{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# High-level flow:\n",
        "  # Load parallel data\n",
        "  # Create tokenizers\n",
        "  # Use seq2seq_model.build_seq2seq_model\n",
        "  # Train & save\n",
        "\n",
        "%cd /content/drive/MyDrive/sanskrit_translation_data\n",
        "!pip install -e .\n",
        "!pip install tensorflow\n",
        "\n",
        "from src.sanskrit_translation.seq2seq_model import build_seq2seq_model\n",
        "from src.sanskrit_translation.paths import SEQ2SEQ_MODEL, TOKENIZER\n",
        "\n",
        "# 1. prepare encoder_input_data, decoder_input_data, decoder_target_data\n",
        "# 2. define num_encoder_tokens, num_decoder_tokens\n",
        "\n",
        "model = build_seq2seq_model(num_encoder_tokens, num_decoder_tokens, latent_dim=256)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=64,\n",
        "    epochs=20,\n",
        "    validation_split=0.1,\n",
        ")\n",
        "\n",
        "model.save(SEQ2SEQ_MODEL)\n",
        "# save tokenizers with pickle to TOKENIZER\n"
      ],
      "metadata": {
        "id": "ZoOY69T0e2B_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}